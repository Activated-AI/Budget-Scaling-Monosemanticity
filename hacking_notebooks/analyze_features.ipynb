{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x74015027e3b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae import TopKSAE, TopKSAEConfig\n",
    "from generate_residuals import EmbeddingGeneratorConfig\n",
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "torch.autograd.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s = torch.load('./saes/sae_no_latent_bias.pt')\n",
    "s = torch.load('./saes/sae_small.pt')\n",
    "sae = TopKSAE(s['config'])\n",
    "sae.load_state_dict(s['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKSAE(\n",
       "  (encode): Linear(in_features=384, out_features=32768, bias=True)\n",
       "  (decode): Linear(in_features=32768, out_features=384, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdconfig = torch.load(\"./small_residuals/0.pt\")['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingGeneratorConfig(batch_size=512, block_size=512, n_embd=384, ratio_tokens_saved=0.07, residual_layer=8, mb_per_save=2000, save_dir='./small_residuals')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dir = embdconfig.save_dir\n",
    "\n",
    "residuals_files = [os.path.join(residuals_dir, f) for f in os.listdir(residuals_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./small_residuals/12.pt'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = torch.load(residuals_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['residuals', 'token_idxs', 'token_values', 'context_window_starts', 'config'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual.keys() # dict_keys(['residuals', 'token_idxs', 'context_window_starts', 'config'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKSAEConfig(embedding_size=384, n_features=32768, topk=24, lr=0.001, batch_size=4096, latent_bias=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_fit_batch(tensor, batch_size):\n",
    "    n = tensor.size(0)\n",
    "    n_batches = n // batch_size\n",
    "    n = n_batches * batch_size\n",
    "    return tensor[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 636/636 [00:04<00:00, 153.40it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def sae_process_residuals(residuals_path, sae):\n",
    "    topk_idxs_per_token = []\n",
    "    topk_strengths_per_token = []\n",
    "\n",
    "    residual = torch.load(residuals_path)\n",
    "    residuals = residual['residuals'].to(torch.float32)\n",
    "    token_idxs = residual['token_idxs']\n",
    "    context_window_starts = residual['context_window_starts']\n",
    "    config = residual['config']\n",
    "    sae.eval()\n",
    "    cropped = crop_fit_batch(residuals, sae.config.batch_size)\n",
    "    batches = cropped.view(-1, sae.config.batch_size, sae.config.embedding_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(batches):\n",
    "            sae_out = sae(batch)\n",
    "            topk_idxs = sae_out['topk_idxs']\n",
    "            topk_values = sae_out['topk_values']\n",
    "            topk_idxs_per_token.append(topk_idxs)\n",
    "            topk_strengths_per_token.append(topk_values)\n",
    "        topk_idxs = torch.cat(topk_idxs_per_token, dim=0)\n",
    "        topk_strengths = torch.cat(topk_strengths_per_token, dim=0)\n",
    "    return topk_idxs, topk_strengths, token_idxs, context_window_starts\n",
    "\n",
    "\n",
    "topk_idxs_per_token, topk_strengths_per_token, dataset_token_location_idxs, dataset_context_window_starts = sae_process_residuals(residuals_files[-1], sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28725, 17068,  6470,  8775,    12, 28371, 18882, 20139, 25526, 23063,\n",
       "         6672, 10003,  3065, 20227,  2933, 25137, 10464, 22462, 18098, 21377,\n",
       "         1265,  7810, 19975, 30799], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_idxs_per_token[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = topk_idxs_per_token[:, 0].view(-1)\n",
    "unique, counts = torch.unique(top_features, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2605056"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.numel(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKSAE(\n",
       "  (encode): Linear(in_features=512, out_features=32768, bias=False)\n",
       "  (decode): Linear(in_features=32768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.2767e-01, 1.7233e-01, 3.8387e-07, 3.8387e-07], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(counts, descending=True)[0][0:10]/torch.numel(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_context_window_starts = torch.tensor(dataset_context_window_starts)\n",
    "dataset_token_location_idxs = torch.tensor(dataset_token_location_idxs)\n",
    "\n",
    "sort_idxs = torch.argsort(dataset_token_location_idxs)\n",
    "sorted_context_window_starts = dataset_context_window_starts[sort_idxs]\n",
    "sorted_token_idxs = dataset_token_location_idxs[sort_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(512, device='cuda:0'), tensor(795, device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 55\n",
    "sorted_context_window_starts[idx], sorted_token_idxs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected feature: tensor(20184, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(20184, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_feature = topk_idxs_per_token.view(-1)[6009]\n",
    "\n",
    "flat_topk_idxs = topk_idxs_per_token.view(-1)\n",
    "\n",
    "random_feature_indexes = torch.where(flat_topk_idxs == random_feature)[0]\n",
    "random_feature_strengths = topk_strengths_per_token.view(-1)[random_feature_indexes]\n",
    "\n",
    "random_feature_subset_strengths, random_feature_subset_flat_feature_idxs = random_feature_strengths.topk(10)\n",
    "# the indexes are relative to the random_feature_indexes(subset of data), we need to map back to the global indexes\n",
    "random_feature_subset_flat_feature_idxs = random_feature_indexes[random_feature_subset_flat_feature_idxs]\n",
    "\n",
    "# random_feature_subset_location_idx\n",
    "random_feature_subset_token_idxs = random_feature_subset_flat_feature_idxs // sae.config.topk\n",
    "random_feature_subset_topk_idxs = random_feature_subset_flat_feature_idxs % sae.config.topk\n",
    "\n",
    "print(\"selected feature:\", random_feature)\n",
    "\n",
    "token_idx = random_feature_subset_token_idxs[1]\n",
    "topk_idx = random_feature_subset_topk_idxs[1]\n",
    "\n",
    "topk_idxs_per_token[token_idx, topk_idx] # as expected, the selected feature via lookup is the same as the random feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping token preprocessing for validation : using cache ./datasets/validation.pt\n",
      "skipping token preprocessing for train : using cache ./datasets/train.pt\n"
     ]
    }
   ],
   "source": [
    "from gpt import preprocess_tokens_from_huggingface\n",
    "import transformers\n",
    "\n",
    "\n",
    "enc = transformers.AutoTokenizer.from_pretrained('activated-ai/tiny-stories-8k-tokenizer')\n",
    "preprocess_tokens_from_huggingface(\"./datasets\")\n",
    "\n",
    "\n",
    "train =  torch.load(\"datasets/train.pt\", map_location=device).to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " each other but with my pistol!\" The other friend< agreed>. They\n",
      "----------\n",
      ". \"It will be fun!\"\n",
      "\n",
      "Ben< agreed>. They\n",
      "----------\n",
      " that they should go on an adventure. The squirrel< agreed>, so\n",
      "----------\n",
      " fight! It will be so much fun!\" Jo< agreed>.\n",
      "\n",
      "----------\n",
      " go on an adventure!â€ Celery nodded< in> agreement and\n",
      "----------\n",
      " see a movie,\" she said. Ben nodded his< head> and smiled\n",
      "----------\n",
      " some more vegetables! They'll be cheap!\" Peter< agreed>, so\n",
      "----------\n",
      "ggy, let us start!\"\n",
      "Iggy barked< in> excitement and\n",
      "----------\n",
      "s go explore the wild!â€ Jim agreed<,> and so\n",
      "----------\n",
      " them to different places.\" Ben says, \"OK<,> that sounds\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "def get_context(token_idx):\n",
    "    start = token_idx - 10\n",
    "    end = token_idx + 3\n",
    "    return enc.decode(train[start:token_idx])+\"<\"+enc.decode(train[token_idx])+\">\"+enc.decode(train[token_idx+1:end])\n",
    "\n",
    "\n",
    "\n",
    "for token_idx in random_feature_subset_token_idxs:\n",
    "    token_idx = token_idx.item()\n",
    "    dataset_token_idx = dataset_token_location_idxs[token_idx]\n",
    "    print(get_context(dataset_token_idx))\n",
    "    print(\"-\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

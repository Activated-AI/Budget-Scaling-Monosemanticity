{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_size: 512\n",
      "n_layer: 14\n",
      "n_head: 16\n",
      "n_embd: 512\n",
      "feed_forward_factor: 2.5\n",
      "vocab_size: 8192\n",
      "data_dir: dataset\n",
      "expt_name: restart_good_3hr_search\n",
      "batch_size: 128\n",
      "max_lr: 0.002\n",
      "min_lr: 0.0001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.99\n",
      "warmup_steps: 50\n",
      "max_steps: 60000\n",
      "max_runtime_seconds: 10800\n",
      "weight_decay: 0.12\n",
      "need_epoch_reshuffle: True\n",
      "matmul_precision: high\n",
      "smoke_test: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7e5fd8086b60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae import TopKSAE, TopKSAEConfig\n",
    "from generate_residuals import EmbeddingGeneratorConfig\n",
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "torch.autograd.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.load('saes/sae.pt')\n",
    "sae = TopKSAE(s['config'])\n",
    "sae.load_state_dict(s['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdconfig = torch.load(\"./residuals/0.pt\")['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingGeneratorConfig(batch_size=512, block_size=512, n_embd=512, ratio_tokens_saved=0.07, residual_layer=10, mb_per_save=2000, save_dir='./residuals/')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dir = embdconfig.save_dir\n",
    "\n",
    "residuals_files = [os.path.join(residuals_dir, f) for f in os.listdir(residuals_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./residuals/16.pt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = torch.load(residuals_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['residuals', 'token_idxs', 'token_values', 'context_window_starts', 'config'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual.keys() # dict_keys(['residuals', 'token_idxs', 'context_window_starts', 'config'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKSAEConfig(embedding_size=512, n_features=32768, topk=24, lr=0.001, batch_size=4096)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_fit_batch(tensor, batch_size):\n",
    "    n = tensor.size(0)\n",
    "    n_batches = n // batch_size\n",
    "    n = n_batches * batch_size\n",
    "    return tensor[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:04<00:00, 96.04it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def sae_process_residuals(residuals_path, sae):\n",
    "    topk_idxs_per_token = []\n",
    "    topk_strengths_per_token = []\n",
    "\n",
    "    residual = torch.load(residuals_path)\n",
    "    residuals = residual['residuals'].to(torch.float32)\n",
    "    token_idxs = residual['token_idxs']\n",
    "    context_window_starts = residual['context_window_starts']\n",
    "    config = residual['config']\n",
    "    sae.eval()\n",
    "    cropped = crop_fit_batch(residuals, sae.config.batch_size)\n",
    "    batches = cropped.view(-1, sae.config.batch_size, sae.config.embedding_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(batches):\n",
    "            sae_out = sae(batch)\n",
    "            topk_idxs = sae_out['topk_idxs']\n",
    "            topk_values = sae_out['topk_values']\n",
    "            topk_idxs_per_token.append(topk_idxs)\n",
    "            topk_strengths_per_token.append(topk_values)\n",
    "        topk_idxs = torch.cat(topk_idxs_per_token, dim=0)\n",
    "        topk_strengths = torch.cat(topk_strengths_per_token, dim=0)\n",
    "    return topk_idxs, topk_strengths, token_idxs, context_window_starts\n",
    "\n",
    "\n",
    "topk_idxs_per_token, topk_strengths_per_token, dataset_token_location_idxs, dataset_context_window_starts = sae_process_residuals(residuals_files[-1], sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_context_window_starts = torch.tensor(dataset_context_window_starts)\n",
    "dataset_token_location_idxs = torch.tensor(dataset_token_location_idxs)\n",
    "\n",
    "sort_idxs = torch.argsort(dataset_token_location_idxs)\n",
    "sorted_context_window_starts = dataset_context_window_starts[sort_idxs]\n",
    "sorted_token_idxs = dataset_token_location_idxs[sort_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(512, device='cuda:0'), tensor(795, device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 55\n",
    "sorted_context_window_starts[idx], sorted_token_idxs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected feature: tensor(20184, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(20184, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_feature = topk_idxs_per_token.view(-1)[6009]\n",
    "\n",
    "flat_topk_idxs = topk_idxs_per_token.view(-1)\n",
    "\n",
    "random_feature_indexes = torch.where(flat_topk_idxs == random_feature)[0]\n",
    "random_feature_strengths = topk_strengths_per_token.view(-1)[random_feature_indexes]\n",
    "\n",
    "random_feature_subset_strengths, random_feature_subset_flat_feature_idxs = random_feature_strengths.topk(10)\n",
    "# the indexes are relative to the random_feature_indexes(subset of data), we need to map back to the global indexes\n",
    "random_feature_subset_flat_feature_idxs = random_feature_indexes[random_feature_subset_flat_feature_idxs]\n",
    "\n",
    "# random_feature_subset_location_idx\n",
    "random_feature_subset_token_idxs = random_feature_subset_flat_feature_idxs // sae.config.topk\n",
    "random_feature_subset_topk_idxs = random_feature_subset_flat_feature_idxs % sae.config.topk\n",
    "\n",
    "print(\"selected feature:\", random_feature)\n",
    "\n",
    "token_idx = random_feature_subset_token_idxs[1]\n",
    "topk_idx = random_feature_subset_topk_idxs[1]\n",
    "\n",
    "topk_idxs_per_token[token_idx, topk_idx] # as expected, the selected feature via lookup is the same as the random feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping token preprocessing for validation : using cache ./datasets/validation.pt\n",
      "skipping token preprocessing for train : using cache ./datasets/train.pt\n"
     ]
    }
   ],
   "source": [
    "from gpt import preprocess_tokens_from_huggingface\n",
    "import transformers\n",
    "\n",
    "\n",
    "enc = transformers.AutoTokenizer.from_pretrained('activated-ai/tiny-stories-8k-tokenizer')\n",
    "preprocess_tokens_from_huggingface(\"./datasets\")\n",
    "\n",
    "\n",
    "train =  torch.load(\"datasets/train.pt\", map_location=device).to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " each other but with my pistol!\" The other friend< agreed>. They\n",
      "----------\n",
      ". \"It will be fun!\"\n",
      "\n",
      "Ben< agreed>. They\n",
      "----------\n",
      " that they should go on an adventure. The squirrel< agreed>, so\n",
      "----------\n",
      " fight! It will be so much fun!\" Jo< agreed>.\n",
      "\n",
      "----------\n",
      " go on an adventure!â€ Celery nodded< in> agreement and\n",
      "----------\n",
      " see a movie,\" she said. Ben nodded his< head> and smiled\n",
      "----------\n",
      " some more vegetables! They'll be cheap!\" Peter< agreed>, so\n",
      "----------\n",
      "ggy, let us start!\"\n",
      "Iggy barked< in> excitement and\n",
      "----------\n",
      "s go explore the wild!â€ Jim agreed<,> and so\n",
      "----------\n",
      " them to different places.\" Ben says, \"OK<,> that sounds\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "def get_context(token_idx):\n",
    "    start = token_idx - 10\n",
    "    end = token_idx + 3\n",
    "    return enc.decode(train[start:token_idx])+\"<\"+enc.decode(train[token_idx])+\">\"+enc.decode(train[token_idx+1:end])\n",
    "\n",
    "\n",
    "\n",
    "for token_idx in random_feature_subset_token_idxs:\n",
    "    token_idx = token_idx.item()\n",
    "    dataset_token_idx = dataset_token_location_idxs[token_idx]\n",
    "    print(get_context(dataset_token_idx))\n",
    "    print(\"-\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

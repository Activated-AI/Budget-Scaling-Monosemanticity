{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mllmhacking\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/diegocaples/Code/budget-scaling-monosemanticity/wandb/run-20240807_193032-0you7h09</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla/runs/0you7h09' target=\"_blank\">playful-jazz-1</a></strong> to <a href='https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla' target=\"_blank\">https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla/runs/0you7h09' target=\"_blank\">https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla/runs/0you7h09</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla/runs/0you7h09?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x70a55bf8eb10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----- imports --------\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    # 'sae_size': 2**18,\n",
    "     'sae_size': 2**14,\n",
    "    \"sae_learning_rate\": 5e-5,\n",
    "    \"sae_sparsity_penalty\": 250,\n",
    "    \"model_embedding_layer\": 6,\n",
    "    \"eval_interval\": 500,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # hidden dimension size\n",
    "    \"B\": 64,\n",
    "    \"T\": 256,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 12,\n",
    "    \"tokenizer_vocab_size\": 2**13,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "# !wandb disabled\n",
    "\n",
    "!wandb enabled\n",
    "wandb.init(\n",
    "   project = \"scaling-monosemanticity-vanilla\",\n",
    "   config = config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoEncoder(nn.Module):\n",
    "    def __init__(self, activations_dim, sparse_dim):\n",
    "        super().__init__()\n",
    "        self.activations_dim = activations_dim\n",
    "        encoder_weight = torch.randn(activations_dim, sparse_dim)\n",
    "        decoder_weight = torch.randn(sparse_dim, activations_dim)\n",
    "        self.encoder_bias = nn.Parameter(torch.zeros(sparse_dim))\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(activations_dim))\n",
    "        self.sparse_dim = sparse_dim\n",
    "        self.sparsity_penalty = sae_sparsity_penalty\n",
    "\n",
    "        # set the encoder_weight to have the activations dim to be normalized to have l2 norm randomly between 0.05 and 1\n",
    "        direction_lengths = torch.rand(sparse_dim) * 0.95 + 0.05\n",
    "        # normalize the encoder_weight along columns (dim -2) to have l2 norm of 1\n",
    "        encoder_weight = F.normalize(encoder_weight, p=2, dim=0)\n",
    "        # multiply the column norms by the direction_lengths\n",
    "        encoder_weight = encoder_weight * direction_lengths\n",
    "        # make the decoder weight be the transpose of the encoder_weight\n",
    "        decoder_weight = torch.transpose(encoder_weight, 0, 1)\n",
    "\n",
    "        self.encoder_weight = nn.Parameter(encoder_weight)\n",
    "        self.decoder_weight = nn.Parameter(decoder_weight)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # preprocessing normalization\n",
    "        # now on average any embedding has euclidian length 1\n",
    "\n",
    "        encoded = F.relu(x @ self.encoder_weight + self.encoder_bias) # all act. are positive\n",
    "        decoded = encoded @ self.decoder_weight + self.decoder_bias\n",
    "\n",
    "        reconstruction_l2_loss = F.mse_loss(x, decoded)\n",
    "\n",
    "        # every row in the tall decoder matrix\n",
    "        # is the \"sum\" of the total influence of a feature on the output\n",
    "        # the l2 norm of that row is the \"influence\" of that feature on that output\n",
    "        # calculate that, store as row\n",
    "        decoder_l2 = torch.linalg.norm(self.decoder_weight, dim=-1)\n",
    "        # the feature activation is the sparse activation * it's influence on output\n",
    "        feature_activations = (encoded) * decoder_l2\n",
    "        # sum of feature activations\n",
    "        # divide by the batch size * sequence length\n",
    "        # should work if there is no batch dimension\n",
    "        if x.ndim == 3:\n",
    "            batch_dim, sequence_dim, _ = x.shape\n",
    "        elif x.ndim == 2:\n",
    "            batch_dim = 1\n",
    "            sequence_dim, _ = x.shape\n",
    "        elif x.ndim == 1:\n",
    "            batch_dim = 1\n",
    "            sequence_dim = 1\n",
    "        else:\n",
    "            raise ValueError(f\"x has {x.ndim} dimensions, but it should have 1, 2, or 3\")\n",
    "        \n",
    "        sparsity_loss = torch.sum(feature_activations) * self.sparsity_penalty / (batch_dim * sequence_dim * self.sparse_dim)\n",
    "\n",
    "        total_loss = reconstruction_l2_loss + sparsity_loss\n",
    "\n",
    "        return {\"encoded\": encoded, \"decoded\": decoded, 'feature_activations': feature_activations, \"reconstruction_loss\": reconstruction_l2_loss, \"sparsity_loss\": sparsity_loss, \"total_loss\": total_loss}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sae = SparseAutoEncoder(C, sae_size)\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=sae_learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 8405248\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in sae.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(filepath):\n",
    "    # load the .pt tensor\n",
    "    tensor = torch.load(filepath)\n",
    "    tensor = torch.cat(tensor, dim=0)\n",
    "    tensor = tensor.to(device)\n",
    "    return tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reconstruction_loss': 2731.3292236328125,\n",
       " 'sparsity_loss': 174.4208755493164,\n",
       " 'total_loss': 2905.75}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_sae_loss(eval_iters, tensor):\n",
    "    sae_loss = 0\n",
    "    sae_sparsity_loss = 0\n",
    "    sae_reconstruction_loss = 0\n",
    "    count = 0\n",
    "    for i in range(0, eval_iters, B):\n",
    "        count += 1\n",
    "        start = i\n",
    "        end = i+B\n",
    "        assert tensor.shape[0] >= end, f\"too many eval_iters\"\n",
    "        sample = tensor[start:end]\n",
    "        sae_output = sae(sample)\n",
    "        sae_loss += sae_output['total_loss'].item()\n",
    "        sae_sparsity_loss += sae_output['sparsity_loss'].item()\n",
    "        sae_reconstruction_loss += sae_output['reconstruction_loss'].item()\n",
    "    avg_loss = sae_loss/count\n",
    "    avg_sparsity_loss = sae_sparsity_loss/count\n",
    "    avg_reconstruction_loss = sae_reconstruction_loss/count\n",
    "    return {\"reconstruction_loss\": avg_reconstruction_loss, \"sparsity_loss\": avg_sparsity_loss, \"total_loss\": avg_loss}\n",
    "    \n",
    "\n",
    "\n",
    "estimate_sae_loss(100, load_tensor(\"residuals/residuals_train_1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = []\n",
    "val_filepaths = []\n",
    "for file in os.listdir(f'residuals'):\n",
    "    if file.startswith(f\"residuals_train\"):\n",
    "        train_filepaths.append(f\"residuals/{file}\")\n",
    "    elif file.startswith(f\"residuals_val\"):\n",
    "        val_filepaths.append(f\"residuals/{file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [02:22<00:00, 359.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:35<00:00, 538.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:57<00:00, 434.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:22<00:00, 619.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [02:10<00:00, 393.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:42<00:00, 500.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44865/44865 [01:46<00:00, 420.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [02:09<00:00, 396.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:42<00:00, 497.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:57<00:00, 435.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:51<00:00, 458.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:36<00:00, 528.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:55<00:00, 443.10it/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbf363314234da0be1b448cc7b55923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>frequent_reconstruction_loss</td><td>█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>frequent_sparsity_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>frequent_total_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>frequent_reconstruction_loss</td><td>0.51637</td></tr><tr><td>frequent_sparsity_loss</td><td>1.82187</td></tr><tr><td>frequent_total_loss</td><td>2.33824</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-jazz-1</strong> at: <a href='https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla/runs/0you7h09' target=\"_blank\">https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla/runs/0you7h09</a><br/> View project at: <a href='https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla' target=\"_blank\">https://wandb.ai/llmhacking/scaling-monosemanticity-vanilla</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240807_193032-0you7h09/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(sae.parameters(), lr=sae_learning_rate)\n",
    "num_epochs = 1\n",
    "logging_interval = 50000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for filepath in train_filepaths:\n",
    "        val_residuals_tensor = load_tensor(random.choice(val_filepaths))\n",
    "        print(f\"val loss on next datafile\")\n",
    "        val_data = estimate_sae_loss(1000, val_residuals_tensor)# keys: reconstruction_loss, sparsity_loss, total_loss\n",
    "        # wandb.log({\"val_reconstruction_loss\": val_data['reconstruction_loss'], \"val_sparsity_loss\": val_data['sparsity_loss'], \"val_total_loss\": val_data['total_loss']})\n",
    "        del val_residuals_tensor\n",
    "        residuals_tensor = load_tensor(filepath)\n",
    "        print(f\"train loss on next datafile\")\n",
    "        train_data = estimate_sae_loss(1000, residuals_tensor)# keys: reconstruction_loss, sparsity_loss, total_loss\n",
    "        # wandb.log({\"train_reconstruction_loss\": train_data['reconstruction_loss'], \"train_sparsity_loss\": train_data['sparsity_loss'], \"train_total_loss\": train_data['total_loss']})\n",
    "        print(f\"training on {filepath}\")\n",
    "\n",
    "        for i in tqdm.tqdm(range(0, residuals_tensor.shape[0]-B, B)):\n",
    "            start = i\n",
    "            end = i+B\n",
    "            assert residuals_tensor.shape[0] >= end, f\"too many train samples\"\n",
    "            sample = residuals_tensor[start:end]\n",
    "            optimizer.zero_grad()\n",
    "            sae_output = sae(sample)\n",
    "            sae_reconstruction_loss = sae_output['reconstruction_loss']\n",
    "            sae_sparsity_loss = sae_output['sparsity_loss']\n",
    "            total_loss = sae_reconstruction_loss + sae_sparsity_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % logging_interval == 0:\n",
    "                pass\n",
    "                wandb.log({\"frequent_reconstruction_loss\": sae_reconstruction_loss, \"frequent_sparsity_loss\": sae_sparsity_loss, \"frequent_total_loss\": total_loss})\n",
    "            \n",
    "wandb.finish()\n",
    "torch.save(sae.state_dict(), 'sae_weights.pth')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights (2**14 size)\n",
    "torch.save(sae.state_dict(), 'sae_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE model loaded from sae_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# Load the SAE model weights\n",
    "sae_weights_path = 'sae_weights.pth'\n",
    "sae.load_state_dict(torch.load(sae_weights_path))\n",
    "sae.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(f\"SAE model loaded from {sae_weights_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

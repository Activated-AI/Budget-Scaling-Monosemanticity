{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# # Specify the split you want to save (e.g., \"train\", \"validation\", \"test\")\n",
    "# split = \"train\"\n",
    "\n",
    "# # Get the desired split from the dataset\n",
    "# subset = dataset[split]\n",
    "\n",
    "# # Save the subset to a text file\n",
    "# subset.to_csv(\"tinystories-train.txt\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- imports --------\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"sae_learning_rate\": 5e-5,\n",
    "    \"model_embedding_layer\": 6,\n",
    "    \"eval_interval\": 500,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # hidden dimension size\n",
    "    \"B\": 64,\n",
    "    \"T\": 256,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 12,\n",
    "    \"tokenizer_vocab_size\": 2**13,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "#wandb.init(\n",
    "#    project = \"tinystories\",\n",
    "#    config = config,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stories_data = []\n",
    "# data_dir = './data'\n",
    "# for filename in os.listdir(data_dir):\n",
    "#     file_path = os.path.join(data_dir, filename)\n",
    "#     if filename.endswith('.json'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             data = json.load(f)\n",
    "#             stories_data.extend(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the tinystories tokenizer\n",
    "# tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "#     \"./tiny-stories-bpe-vocab.json\", \n",
    "#     \"./tiny-stories-bpe-merges.txt\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# def encode(text):\n",
    "#     return torch.tensor(tokenizer.encode(text).ids, dtype=torch.int64)\n",
    "# def decode(encoded_text):\n",
    "#     return tokenizer.decode(encoded_text.tolist())\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# encoded_stories = [encode(story['story']) for story in tqdm(stories_data, desc=\"Encoding stories\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the encoded stories to a file\n",
    "# torch.save(encoded_stories, 'encoded-stories.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('tinystories-train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1916206969\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479051742.25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1916206969/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in lines:  20550005\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in lines: \", len(text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "\"One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"\"Mom, I found this needle. Can you share it with me and sew my shirt?\"\" Her mom smiled and said, \"\"Yes, Lily, we can share the needle and fix your shirt.\"\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\"\n",
      "\"Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n",
      "\n",
      "One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that wer\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = ['tinystories-train.txt']\n",
    "# tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "\n",
    "# tokenizer.train(files=paths, vocab_size=tokenizer_vocab_size, min_frequency=2)\n",
    "\n",
    "# tokenizer.save_model('.', 'tiny-stories-bpe')\n",
    "\n",
    "\n",
    "\n",
    "# enc = tokenizer.encode(\"She sells sea shells by the sea shore!\")\n",
    "# tokenizer.decode(enc.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    \"./tiny-stories-bpe-vocab.json\", \n",
    "    \"./tiny-stories-bpe-merges.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6132]\n",
      "hello\n",
      "vocab size:  8192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_encode(text, batch_size):\n",
    "    tokens = []\n",
    "    for i in tqdm(range(0, len(text), batch_size)):\n",
    "        tokens.extend(encode(text[i:i+batch_size]))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "hello_encoded = encode(\"hello\")\n",
    "print(hello_encoded)\n",
    "print(decode(hello_encoded))\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 66.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory used by sample_encoded:  1.2918853759765625 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = text[:200000]\n",
    "sample_encoded = batch_encode(sample_text, 20000)\n",
    "\n",
    "# get the amount of memory used by sample_encoded\n",
    "def recursive_memory_usage(python_obj):\n",
    "    if isinstance(python_obj, (str, int, float)):\n",
    "        return python_obj.__sizeof__()\n",
    "    if isinstance(python_obj, dict):\n",
    "        return sum([recursive_memory_usage(v) for v in python_obj.values()])\n",
    "    if isinstance(python_obj, list):\n",
    "        return sum([recursive_memory_usage(v) for v in python_obj])\n",
    "    return python_obj.__sizeof__()\n",
    "\n",
    "print(\"memory used by sample_encoded: \", recursive_memory_usage(sample_encoded) / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  10000\n",
      "length of dataset in tokens:  2457\n",
      "characters per token:  4.07000407000407\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text[:10000]))\n",
    "print(\"length of dataset in tokens: \", len(encode(text[:10000])))\n",
    "chars_per_token = len(text[:10000]) / len(encode(text[:10000]))\n",
    "print(\"characters per token: \", chars_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_text = batch_encode(text, 200000)\n",
    "# # data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "# data = torch.tensor(encoded_text, dtype=torch.int64, device='cuda')\n",
    "# print(data.dtype)\n",
    "# print(data.size())\n",
    "# print(data.device)\n",
    "# torch.save(data, 'tiny-stories-train.pt')\n",
    "# encoded_text = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from tiny-stories-train.pt\n",
    "data = torch.load('tiny-stories-train.pt', map_location='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468832276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([421949048])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  83, 3206,  198,    1,  421,  356,   11,  258,  397,  447,  501,  364,\n",
       "         596,  258, 3736,  316,  309,  759,   13,  313,  704,  304,  282, 2966,\n",
       "         265,  359,  342,  304,  788,  304,  282, 2120,   13,  364,  445,  265,\n",
       "         949,  262, 3736,  342,  309,  365,   11,  350,  338,  461, 5198,  258,\n",
       "        2228,  345,  309, 2500,   13,  198,  198,  343,  469,  265,  309,  365,\n",
       "         264,  327,   11,  329,  771,   11,  335,  596,  741, 3736,   13, 1282,\n",
       "         346,  949,  304,  342,  519,  264, 5198,  652, 2500,  478,  866,  365,\n",
       "         499,  264,  327,   11,  329,  832,   11,  364,   11,  363,  472,  949,\n",
       "         262, 3736,  264, 1306,  627, 2500,  416,  198,  198, 4625,   11,  362,\n",
       "        1656,  262, 3736,  264, 7930,  262, 2228,  345,  364,  371, 2500,   13,\n",
       "         410,  282,  385, 2966,  366,  449,  788,  362,  430, 2502,  264, 1762,\n",
       "         757,  573,   13, 1453,  362, 1444,   11,  364,  858,  309,  365,  366,\n",
       "        2502,  262, 3736,  264, 5150,  309, 2500,   13,  320,  897,  514,  405,\n",
       "         788,  362,  360, 1656,  264, 1370,  567,  408,  198,    1,  432,  448,\n",
       "         258,  396,   11,  400,  282,  258,  397,  565,  501, 2632,  592,   13,\n",
       "        2632,  592,  504,  265,  437,  848,  264,  359,  316,  262,  734,   13,\n",
       "        2632,  592,  282,  258, 2188,  565,  788,  278,  667,  360,  590, 4830,\n",
       "          13, 5204, 4830,  564, 2632,  592,  405,  264, 1123,   13,  198,  198,\n",
       "         421,  356,   11, 2632,  592,  282, 3476,  316,  262,  570,  589,  278,\n",
       "         414,  258,  407,  680,   13,  299,  680,  360,  791, 1508,  357,  430,\n",
       "        4175,   13, 2632,  592,  616,  713,  262, 1508, 1544,  264,  445,  265,\n",
       "         359,  342,  449,   13, 2632], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text\\n\"One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"\"Mom, I found this needle. Can you share it with me and sew my shirt?\"\" Her mom smiled and said, \"\"Yes, Lily, we can share the needle and fix your shirt.\"\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\"\\n\"Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Be'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[:T+1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:T]\n",
    "y = train_data[1:T+1]\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(\"when we see the text\", context, \"we predict the next character is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, data.size(0) - T, (B,)) # 4 random locations we can sample from\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T): # for each of the characters in the sample\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "        # xbow = wei @ x # apply the mask to the input, bag of words because simple avg.\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)\n",
    "# head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2082, -0.4307, -1.4964,  ..., -0.4924,  0.3247,  0.0516],\n",
       "          [ 0.3258,  0.6623, -1.7060,  ..., -1.1121,  0.1928, -1.1646],\n",
       "          [ 1.4009,  0.4413, -1.9436,  ..., -1.1725,  1.1626, -1.2866],\n",
       "          ...,\n",
       "          [-1.0671, -0.6851, -1.0190,  ..., -1.7658,  0.0694, -1.3291],\n",
       "          [-1.2112, -0.6265, -1.5360,  ..., -1.1472, -0.9685, -2.1975],\n",
       "          [-0.3226,  0.3611, -0.4603,  ..., -1.1162,  0.3319, -1.2689]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " None,\n",
       " tensor(0., device='cuda:0'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, C) \n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_head = nn.Linear(C, vocab_size)\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, idx, targets=None, return_residuals=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        if return_residuals == \"first_embedding\":\n",
    "            return x\n",
    "\n",
    "        # def excess_kurtosis(emb):\n",
    "        #     mean = torch.mean(emb, dim=-1, keepdim=True) # BxTx1\n",
    "        #     std = torch.std(emb, dim=-1, keepdim=True) # BxTx1\n",
    "\n",
    "        #     centralized = emb - mean #BxTxC\n",
    "        #     fourth_moment = torch.mean(centralized**4, dim=-1, keepdim=True) # BxTx1\n",
    "        #     kurtosis = torch.squeeze(fourth_moment / std**4, dim=-1) # BxT\n",
    "        #     # view as a 1d vector\n",
    "        #     kurtosis = kurtosis.view(-1) - 3\n",
    "        #     # make each one min 0\n",
    "        #     kurtosis = torch.maximum(kurtosis, torch.tensor(0.0))\n",
    "        #     # sum over the vector\n",
    "        #     kurtosis = torch.sum(kurtosis)\n",
    "        #     return kurtosis\n",
    "\n",
    "\n",
    "        # kurtosis_sum = torch.tensor(0.0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # kurtosis_sum += excess_kurtosis(x)\n",
    "            if return_residuals is not None and i == return_residuals:\n",
    "                return x\n",
    "        \n",
    "        # kurtosis_avg = kurtosis_sum / (len(self.layers) * T * B)\n",
    "        kurtosis_avg = torch.tensor(0.0)\n",
    "\n",
    "        logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None, kurtosis_avg\n",
    "        else:\n",
    "            # a list of all the predictions, reguardles of batch.\n",
    "            # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "            # ydim: all predictions for all batches flattened (batch_dim*sequence_dim)\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            # targets loss view\n",
    "            # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "            # so this would be like, [1,4,5,1,2,3, ...]\n",
    "            # where each number is the correct next index of the one hot vector\n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss, kurtosis_avg\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=0.5):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx[:,-T:])\n",
    "            # get the predictions of the last token\n",
    "            last_token_logits = logits[:, -1, :] # all batches, last token, all probabilities\n",
    "            # apply temperature\n",
    "            last_token_logits = last_token_logits / temperature\n",
    "            # softmax to get probabilities\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            # sample from the probabilities\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # add the new token to the idx tensor\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    def prompt_model(self, prompt, max_new_tokens, temperature=0.5):\n",
    "        autoregressive_seq = encode(prompt)\n",
    "        for _ in range(max_new_tokens):\n",
    "            prediction_index = len(autoregressive_seq)-1\n",
    "\n",
    "            model_input = torch.tensor(autoregressive_seq)\n",
    "            \n",
    "            while model_input.shape[0] < T:\n",
    "                pad_token = torch.tensor(encode(\"\\n\"))\n",
    "                model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "\n",
    "            model_input\n",
    "            model_input = model_input.unsqueeze(0)\n",
    "\n",
    "            logits, loss, kurtosis_avg = model(model_input)\n",
    "            prediction_token = logits[:, prediction_index, :] / temperature\n",
    "            probabilities = F.softmax(prediction_token, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            next_token = next_token.item()\n",
    "\n",
    "            autoregressive_seq.append(next_token)\n",
    "        # get the autoregressive sequence\n",
    "        return decode(autoregressive_seq)\n",
    "    def get_embedding(self, prompt, override_model_embedding_layer=None):\n",
    "        if override_model_embedding_layer is None:\n",
    "            selected_model_embedding_layer = model_embedding_layer\n",
    "        else:\n",
    "            selected_model_embedding_layer = override_model_embedding_layer\n",
    "        sequence = encode(prompt)\n",
    "        model_input = torch.tensor(sequence)\n",
    "        sequence_index = len(sequence) - 1\n",
    "        while model_input.shape[0] < T:\n",
    "            pad_token = torch.tensor(encode(\"\\n\"))\n",
    "            model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "        model_input = model_input.unsqueeze(0)\n",
    "        embedding = self.forward(model_input, return_residuals=selected_model_embedding_layer)\n",
    "        # remove the batch dimension\n",
    "        embedding = embedding.squeeze(0)[sequence_index]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "model = GPT(n_layers)\n",
    "# logits, loss, kurtosis_avg = model(xb, yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "# print(kurtosis_avg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_idx = torch.zeros(1, T).long()\n",
    "model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(8192, 256)\n",
       "  (position_embedding_table): Embedding(256, 256)\n",
       "  (lm_head): Linear(in_features=256, out_features=8192, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (combine_heads): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in the model:  12160000\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of parameters in the model: \", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([468832276])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits, loss = self(idx[:,-T:])\n",
    "\n",
    "idx = torch.zeros(1, 1).long()\n",
    "idx[:,-T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_embedding_table.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss, kurtosis_avg = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter_to_data_ratio=0.028818645420903996\n",
      "token_embedding_table.weight: 2097152\n",
      "lm_head.weight: 2097152\n",
      "layers.0.ff.net.0.weight: 196608\n",
      "layers.0.ff.net.2.weight: 196608\n",
      "layers.1.ff.net.0.weight: 196608\n",
      "layers.1.ff.net.2.weight: 196608\n",
      "layers.2.ff.net.0.weight: 196608\n",
      "layers.2.ff.net.2.weight: 196608\n",
      "layers.3.ff.net.0.weight: 196608\n",
      "layers.3.ff.net.2.weight: 196608\n",
      "layers.4.ff.net.0.weight: 196608\n",
      "layers.4.ff.net.2.weight: 196608\n",
      "layers.5.ff.net.0.weight: 196608\n",
      "layers.5.ff.net.2.weight: 196608\n",
      "layers.6.ff.net.0.weight: 196608\n",
      "layers.6.ff.net.2.weight: 196608\n",
      "layers.7.ff.net.0.weight: 196608\n",
      "layers.7.ff.net.2.weight: 196608\n",
      "layers.8.ff.net.0.weight: 196608\n",
      "layers.8.ff.net.2.weight: 196608\n",
      "layers.9.ff.net.0.weight: 196608\n",
      "layers.9.ff.net.2.weight: 196608\n",
      "layers.10.ff.net.0.weight: 196608\n",
      "layers.10.ff.net.2.weight: 196608\n",
      "layers.11.ff.net.0.weight: 196608\n",
      "layers.11.ff.net.2.weight: 196608\n",
      "position_embedding_table.weight: 65536\n",
      "layers.0.attention.combine_heads.weight: 65536\n",
      "layers.1.attention.combine_heads.weight: 65536\n",
      "layers.2.attention.combine_heads.weight: 65536\n",
      "layers.3.attention.combine_heads.weight: 65536\n",
      "layers.4.attention.combine_heads.weight: 65536\n",
      "layers.5.attention.combine_heads.weight: 65536\n",
      "layers.6.attention.combine_heads.weight: 65536\n",
      "layers.7.attention.combine_heads.weight: 65536\n",
      "layers.8.attention.combine_heads.weight: 65536\n",
      "layers.9.attention.combine_heads.weight: 65536\n",
      "layers.10.attention.combine_heads.weight: 65536\n",
      "layers.11.attention.combine_heads.weight: 65536\n",
      "lm_head.bias: 8192\n",
      "layers.0.attention.heads.0.query.weight: 8192\n",
      "layers.0.attention.heads.0.key.weight: 8192\n",
      "layers.0.attention.heads.0.value.weight: 8192\n",
      "layers.0.attention.heads.1.query.weight: 8192\n",
      "layers.0.attention.heads.1.key.weight: 8192\n",
      "layers.0.attention.heads.1.value.weight: 8192\n",
      "layers.0.attention.heads.2.query.weight: 8192\n",
      "layers.0.attention.heads.2.key.weight: 8192\n",
      "layers.0.attention.heads.2.value.weight: 8192\n",
      "layers.0.attention.heads.3.query.weight: 8192\n",
      "layers.0.attention.heads.3.key.weight: 8192\n",
      "layers.0.attention.heads.3.value.weight: 8192\n",
      "layers.0.attention.heads.4.query.weight: 8192\n",
      "layers.0.attention.heads.4.key.weight: 8192\n",
      "layers.0.attention.heads.4.value.weight: 8192\n",
      "layers.0.attention.heads.5.query.weight: 8192\n",
      "layers.0.attention.heads.5.key.weight: 8192\n",
      "layers.0.attention.heads.5.value.weight: 8192\n",
      "layers.0.attention.heads.6.query.weight: 8192\n",
      "layers.0.attention.heads.6.key.weight: 8192\n",
      "layers.0.attention.heads.6.value.weight: 8192\n",
      "layers.0.attention.heads.7.query.weight: 8192\n",
      "layers.0.attention.heads.7.key.weight: 8192\n",
      "layers.0.attention.heads.7.value.weight: 8192\n",
      "layers.1.attention.heads.0.query.weight: 8192\n",
      "layers.1.attention.heads.0.key.weight: 8192\n",
      "layers.1.attention.heads.0.value.weight: 8192\n",
      "layers.1.attention.heads.1.query.weight: 8192\n",
      "layers.1.attention.heads.1.key.weight: 8192\n",
      "layers.1.attention.heads.1.value.weight: 8192\n",
      "layers.1.attention.heads.2.query.weight: 8192\n",
      "layers.1.attention.heads.2.key.weight: 8192\n",
      "layers.1.attention.heads.2.value.weight: 8192\n",
      "layers.1.attention.heads.3.query.weight: 8192\n",
      "layers.1.attention.heads.3.key.weight: 8192\n",
      "layers.1.attention.heads.3.value.weight: 8192\n",
      "layers.1.attention.heads.4.query.weight: 8192\n",
      "layers.1.attention.heads.4.key.weight: 8192\n",
      "layers.1.attention.heads.4.value.weight: 8192\n",
      "layers.1.attention.heads.5.query.weight: 8192\n",
      "layers.1.attention.heads.5.key.weight: 8192\n",
      "layers.1.attention.heads.5.value.weight: 8192\n",
      "layers.1.attention.heads.6.query.weight: 8192\n",
      "layers.1.attention.heads.6.key.weight: 8192\n",
      "layers.1.attention.heads.6.value.weight: 8192\n",
      "layers.1.attention.heads.7.query.weight: 8192\n",
      "layers.1.attention.heads.7.key.weight: 8192\n",
      "layers.1.attention.heads.7.value.weight: 8192\n",
      "layers.2.attention.heads.0.query.weight: 8192\n",
      "layers.2.attention.heads.0.key.weight: 8192\n",
      "layers.2.attention.heads.0.value.weight: 8192\n",
      "layers.2.attention.heads.1.query.weight: 8192\n",
      "layers.2.attention.heads.1.key.weight: 8192\n",
      "layers.2.attention.heads.1.value.weight: 8192\n",
      "layers.2.attention.heads.2.query.weight: 8192\n",
      "layers.2.attention.heads.2.key.weight: 8192\n",
      "layers.2.attention.heads.2.value.weight: 8192\n",
      "layers.2.attention.heads.3.query.weight: 8192\n",
      "layers.2.attention.heads.3.key.weight: 8192\n",
      "layers.2.attention.heads.3.value.weight: 8192\n",
      "layers.2.attention.heads.4.query.weight: 8192\n",
      "layers.2.attention.heads.4.key.weight: 8192\n",
      "layers.2.attention.heads.4.value.weight: 8192\n",
      "layers.2.attention.heads.5.query.weight: 8192\n",
      "layers.2.attention.heads.5.key.weight: 8192\n",
      "layers.2.attention.heads.5.value.weight: 8192\n",
      "layers.2.attention.heads.6.query.weight: 8192\n",
      "layers.2.attention.heads.6.key.weight: 8192\n",
      "layers.2.attention.heads.6.value.weight: 8192\n",
      "layers.2.attention.heads.7.query.weight: 8192\n",
      "layers.2.attention.heads.7.key.weight: 8192\n",
      "layers.2.attention.heads.7.value.weight: 8192\n",
      "layers.3.attention.heads.0.query.weight: 8192\n",
      "layers.3.attention.heads.0.key.weight: 8192\n",
      "layers.3.attention.heads.0.value.weight: 8192\n",
      "layers.3.attention.heads.1.query.weight: 8192\n",
      "layers.3.attention.heads.1.key.weight: 8192\n",
      "layers.3.attention.heads.1.value.weight: 8192\n",
      "layers.3.attention.heads.2.query.weight: 8192\n",
      "layers.3.attention.heads.2.key.weight: 8192\n",
      "layers.3.attention.heads.2.value.weight: 8192\n",
      "layers.3.attention.heads.3.query.weight: 8192\n",
      "layers.3.attention.heads.3.key.weight: 8192\n",
      "layers.3.attention.heads.3.value.weight: 8192\n",
      "layers.3.attention.heads.4.query.weight: 8192\n",
      "layers.3.attention.heads.4.key.weight: 8192\n",
      "layers.3.attention.heads.4.value.weight: 8192\n",
      "layers.3.attention.heads.5.query.weight: 8192\n",
      "layers.3.attention.heads.5.key.weight: 8192\n",
      "layers.3.attention.heads.5.value.weight: 8192\n",
      "layers.3.attention.heads.6.query.weight: 8192\n",
      "layers.3.attention.heads.6.key.weight: 8192\n",
      "layers.3.attention.heads.6.value.weight: 8192\n",
      "layers.3.attention.heads.7.query.weight: 8192\n",
      "layers.3.attention.heads.7.key.weight: 8192\n",
      "layers.3.attention.heads.7.value.weight: 8192\n",
      "layers.4.attention.heads.0.query.weight: 8192\n",
      "layers.4.attention.heads.0.key.weight: 8192\n",
      "layers.4.attention.heads.0.value.weight: 8192\n",
      "layers.4.attention.heads.1.query.weight: 8192\n",
      "layers.4.attention.heads.1.key.weight: 8192\n",
      "layers.4.attention.heads.1.value.weight: 8192\n",
      "layers.4.attention.heads.2.query.weight: 8192\n",
      "layers.4.attention.heads.2.key.weight: 8192\n",
      "layers.4.attention.heads.2.value.weight: 8192\n",
      "layers.4.attention.heads.3.query.weight: 8192\n",
      "layers.4.attention.heads.3.key.weight: 8192\n",
      "layers.4.attention.heads.3.value.weight: 8192\n",
      "layers.4.attention.heads.4.query.weight: 8192\n",
      "layers.4.attention.heads.4.key.weight: 8192\n",
      "layers.4.attention.heads.4.value.weight: 8192\n",
      "layers.4.attention.heads.5.query.weight: 8192\n",
      "layers.4.attention.heads.5.key.weight: 8192\n",
      "layers.4.attention.heads.5.value.weight: 8192\n",
      "layers.4.attention.heads.6.query.weight: 8192\n",
      "layers.4.attention.heads.6.key.weight: 8192\n",
      "layers.4.attention.heads.6.value.weight: 8192\n",
      "layers.4.attention.heads.7.query.weight: 8192\n",
      "layers.4.attention.heads.7.key.weight: 8192\n",
      "layers.4.attention.heads.7.value.weight: 8192\n",
      "layers.5.attention.heads.0.query.weight: 8192\n",
      "layers.5.attention.heads.0.key.weight: 8192\n",
      "layers.5.attention.heads.0.value.weight: 8192\n",
      "layers.5.attention.heads.1.query.weight: 8192\n",
      "layers.5.attention.heads.1.key.weight: 8192\n",
      "layers.5.attention.heads.1.value.weight: 8192\n",
      "layers.5.attention.heads.2.query.weight: 8192\n",
      "layers.5.attention.heads.2.key.weight: 8192\n",
      "layers.5.attention.heads.2.value.weight: 8192\n",
      "layers.5.attention.heads.3.query.weight: 8192\n",
      "layers.5.attention.heads.3.key.weight: 8192\n",
      "layers.5.attention.heads.3.value.weight: 8192\n",
      "layers.5.attention.heads.4.query.weight: 8192\n",
      "layers.5.attention.heads.4.key.weight: 8192\n",
      "layers.5.attention.heads.4.value.weight: 8192\n",
      "layers.5.attention.heads.5.query.weight: 8192\n",
      "layers.5.attention.heads.5.key.weight: 8192\n",
      "layers.5.attention.heads.5.value.weight: 8192\n",
      "layers.5.attention.heads.6.query.weight: 8192\n",
      "layers.5.attention.heads.6.key.weight: 8192\n",
      "layers.5.attention.heads.6.value.weight: 8192\n",
      "layers.5.attention.heads.7.query.weight: 8192\n",
      "layers.5.attention.heads.7.key.weight: 8192\n",
      "layers.5.attention.heads.7.value.weight: 8192\n",
      "layers.6.attention.heads.0.query.weight: 8192\n",
      "layers.6.attention.heads.0.key.weight: 8192\n",
      "layers.6.attention.heads.0.value.weight: 8192\n",
      "layers.6.attention.heads.1.query.weight: 8192\n",
      "layers.6.attention.heads.1.key.weight: 8192\n",
      "layers.6.attention.heads.1.value.weight: 8192\n",
      "layers.6.attention.heads.2.query.weight: 8192\n",
      "layers.6.attention.heads.2.key.weight: 8192\n",
      "layers.6.attention.heads.2.value.weight: 8192\n",
      "layers.6.attention.heads.3.query.weight: 8192\n",
      "layers.6.attention.heads.3.key.weight: 8192\n",
      "layers.6.attention.heads.3.value.weight: 8192\n",
      "layers.6.attention.heads.4.query.weight: 8192\n",
      "layers.6.attention.heads.4.key.weight: 8192\n",
      "layers.6.attention.heads.4.value.weight: 8192\n",
      "layers.6.attention.heads.5.query.weight: 8192\n",
      "layers.6.attention.heads.5.key.weight: 8192\n",
      "layers.6.attention.heads.5.value.weight: 8192\n",
      "layers.6.attention.heads.6.query.weight: 8192\n",
      "layers.6.attention.heads.6.key.weight: 8192\n",
      "layers.6.attention.heads.6.value.weight: 8192\n",
      "layers.6.attention.heads.7.query.weight: 8192\n",
      "layers.6.attention.heads.7.key.weight: 8192\n",
      "layers.6.attention.heads.7.value.weight: 8192\n",
      "layers.7.attention.heads.0.query.weight: 8192\n",
      "layers.7.attention.heads.0.key.weight: 8192\n",
      "layers.7.attention.heads.0.value.weight: 8192\n",
      "layers.7.attention.heads.1.query.weight: 8192\n",
      "layers.7.attention.heads.1.key.weight: 8192\n",
      "layers.7.attention.heads.1.value.weight: 8192\n",
      "layers.7.attention.heads.2.query.weight: 8192\n",
      "layers.7.attention.heads.2.key.weight: 8192\n",
      "layers.7.attention.heads.2.value.weight: 8192\n",
      "layers.7.attention.heads.3.query.weight: 8192\n",
      "layers.7.attention.heads.3.key.weight: 8192\n",
      "layers.7.attention.heads.3.value.weight: 8192\n",
      "layers.7.attention.heads.4.query.weight: 8192\n",
      "layers.7.attention.heads.4.key.weight: 8192\n",
      "layers.7.attention.heads.4.value.weight: 8192\n",
      "layers.7.attention.heads.5.query.weight: 8192\n",
      "layers.7.attention.heads.5.key.weight: 8192\n",
      "layers.7.attention.heads.5.value.weight: 8192\n",
      "layers.7.attention.heads.6.query.weight: 8192\n",
      "layers.7.attention.heads.6.key.weight: 8192\n",
      "layers.7.attention.heads.6.value.weight: 8192\n",
      "layers.7.attention.heads.7.query.weight: 8192\n",
      "layers.7.attention.heads.7.key.weight: 8192\n",
      "layers.7.attention.heads.7.value.weight: 8192\n",
      "layers.8.attention.heads.0.query.weight: 8192\n",
      "layers.8.attention.heads.0.key.weight: 8192\n",
      "layers.8.attention.heads.0.value.weight: 8192\n",
      "layers.8.attention.heads.1.query.weight: 8192\n",
      "layers.8.attention.heads.1.key.weight: 8192\n",
      "layers.8.attention.heads.1.value.weight: 8192\n",
      "layers.8.attention.heads.2.query.weight: 8192\n",
      "layers.8.attention.heads.2.key.weight: 8192\n",
      "layers.8.attention.heads.2.value.weight: 8192\n",
      "layers.8.attention.heads.3.query.weight: 8192\n",
      "layers.8.attention.heads.3.key.weight: 8192\n",
      "layers.8.attention.heads.3.value.weight: 8192\n",
      "layers.8.attention.heads.4.query.weight: 8192\n",
      "layers.8.attention.heads.4.key.weight: 8192\n",
      "layers.8.attention.heads.4.value.weight: 8192\n",
      "layers.8.attention.heads.5.query.weight: 8192\n",
      "layers.8.attention.heads.5.key.weight: 8192\n",
      "layers.8.attention.heads.5.value.weight: 8192\n",
      "layers.8.attention.heads.6.query.weight: 8192\n",
      "layers.8.attention.heads.6.key.weight: 8192\n",
      "layers.8.attention.heads.6.value.weight: 8192\n",
      "layers.8.attention.heads.7.query.weight: 8192\n",
      "layers.8.attention.heads.7.key.weight: 8192\n",
      "layers.8.attention.heads.7.value.weight: 8192\n",
      "layers.9.attention.heads.0.query.weight: 8192\n",
      "layers.9.attention.heads.0.key.weight: 8192\n",
      "layers.9.attention.heads.0.value.weight: 8192\n",
      "layers.9.attention.heads.1.query.weight: 8192\n",
      "layers.9.attention.heads.1.key.weight: 8192\n",
      "layers.9.attention.heads.1.value.weight: 8192\n",
      "layers.9.attention.heads.2.query.weight: 8192\n",
      "layers.9.attention.heads.2.key.weight: 8192\n",
      "layers.9.attention.heads.2.value.weight: 8192\n",
      "layers.9.attention.heads.3.query.weight: 8192\n",
      "layers.9.attention.heads.3.key.weight: 8192\n",
      "layers.9.attention.heads.3.value.weight: 8192\n",
      "layers.9.attention.heads.4.query.weight: 8192\n",
      "layers.9.attention.heads.4.key.weight: 8192\n",
      "layers.9.attention.heads.4.value.weight: 8192\n",
      "layers.9.attention.heads.5.query.weight: 8192\n",
      "layers.9.attention.heads.5.key.weight: 8192\n",
      "layers.9.attention.heads.5.value.weight: 8192\n",
      "layers.9.attention.heads.6.query.weight: 8192\n",
      "layers.9.attention.heads.6.key.weight: 8192\n",
      "layers.9.attention.heads.6.value.weight: 8192\n",
      "layers.9.attention.heads.7.query.weight: 8192\n",
      "layers.9.attention.heads.7.key.weight: 8192\n",
      "layers.9.attention.heads.7.value.weight: 8192\n",
      "layers.10.attention.heads.0.query.weight: 8192\n",
      "layers.10.attention.heads.0.key.weight: 8192\n",
      "layers.10.attention.heads.0.value.weight: 8192\n",
      "layers.10.attention.heads.1.query.weight: 8192\n",
      "layers.10.attention.heads.1.key.weight: 8192\n",
      "layers.10.attention.heads.1.value.weight: 8192\n",
      "layers.10.attention.heads.2.query.weight: 8192\n",
      "layers.10.attention.heads.2.key.weight: 8192\n",
      "layers.10.attention.heads.2.value.weight: 8192\n",
      "layers.10.attention.heads.3.query.weight: 8192\n",
      "layers.10.attention.heads.3.key.weight: 8192\n",
      "layers.10.attention.heads.3.value.weight: 8192\n",
      "layers.10.attention.heads.4.query.weight: 8192\n",
      "layers.10.attention.heads.4.key.weight: 8192\n",
      "layers.10.attention.heads.4.value.weight: 8192\n",
      "layers.10.attention.heads.5.query.weight: 8192\n",
      "layers.10.attention.heads.5.key.weight: 8192\n",
      "layers.10.attention.heads.5.value.weight: 8192\n",
      "layers.10.attention.heads.6.query.weight: 8192\n",
      "layers.10.attention.heads.6.key.weight: 8192\n",
      "layers.10.attention.heads.6.value.weight: 8192\n",
      "layers.10.attention.heads.7.query.weight: 8192\n",
      "layers.10.attention.heads.7.key.weight: 8192\n",
      "layers.10.attention.heads.7.value.weight: 8192\n",
      "layers.11.attention.heads.0.query.weight: 8192\n",
      "layers.11.attention.heads.0.key.weight: 8192\n",
      "layers.11.attention.heads.0.value.weight: 8192\n",
      "layers.11.attention.heads.1.query.weight: 8192\n",
      "layers.11.attention.heads.1.key.weight: 8192\n",
      "layers.11.attention.heads.1.value.weight: 8192\n",
      "layers.11.attention.heads.2.query.weight: 8192\n",
      "layers.11.attention.heads.2.key.weight: 8192\n",
      "layers.11.attention.heads.2.value.weight: 8192\n",
      "layers.11.attention.heads.3.query.weight: 8192\n",
      "layers.11.attention.heads.3.key.weight: 8192\n",
      "layers.11.attention.heads.3.value.weight: 8192\n",
      "layers.11.attention.heads.4.query.weight: 8192\n",
      "layers.11.attention.heads.4.key.weight: 8192\n",
      "layers.11.attention.heads.4.value.weight: 8192\n",
      "layers.11.attention.heads.5.query.weight: 8192\n",
      "layers.11.attention.heads.5.key.weight: 8192\n",
      "layers.11.attention.heads.5.value.weight: 8192\n",
      "layers.11.attention.heads.6.query.weight: 8192\n",
      "layers.11.attention.heads.6.key.weight: 8192\n",
      "layers.11.attention.heads.6.value.weight: 8192\n",
      "layers.11.attention.heads.7.query.weight: 8192\n",
      "layers.11.attention.heads.7.key.weight: 8192\n",
      "layers.11.attention.heads.7.value.weight: 8192\n",
      "layers.0.ff.net.0.bias: 768\n",
      "layers.1.ff.net.0.bias: 768\n",
      "layers.2.ff.net.0.bias: 768\n",
      "layers.3.ff.net.0.bias: 768\n",
      "layers.4.ff.net.0.bias: 768\n",
      "layers.5.ff.net.0.bias: 768\n",
      "layers.6.ff.net.0.bias: 768\n",
      "layers.7.ff.net.0.bias: 768\n",
      "layers.8.ff.net.0.bias: 768\n",
      "layers.9.ff.net.0.bias: 768\n",
      "layers.10.ff.net.0.bias: 768\n",
      "layers.11.ff.net.0.bias: 768\n",
      "layers.0.attention.combine_heads.bias: 256\n",
      "layers.0.ff.net.2.bias: 256\n",
      "layers.0.norm1.gamma: 256\n",
      "layers.0.norm1.beta: 256\n",
      "layers.0.norm2.gamma: 256\n",
      "layers.0.norm2.beta: 256\n",
      "layers.1.attention.combine_heads.bias: 256\n",
      "layers.1.ff.net.2.bias: 256\n",
      "layers.1.norm1.gamma: 256\n",
      "layers.1.norm1.beta: 256\n",
      "layers.1.norm2.gamma: 256\n",
      "layers.1.norm2.beta: 256\n",
      "layers.2.attention.combine_heads.bias: 256\n",
      "layers.2.ff.net.2.bias: 256\n",
      "layers.2.norm1.gamma: 256\n",
      "layers.2.norm1.beta: 256\n",
      "layers.2.norm2.gamma: 256\n",
      "layers.2.norm2.beta: 256\n",
      "layers.3.attention.combine_heads.bias: 256\n",
      "layers.3.ff.net.2.bias: 256\n",
      "layers.3.norm1.gamma: 256\n",
      "layers.3.norm1.beta: 256\n",
      "layers.3.norm2.gamma: 256\n",
      "layers.3.norm2.beta: 256\n",
      "layers.4.attention.combine_heads.bias: 256\n",
      "layers.4.ff.net.2.bias: 256\n",
      "layers.4.norm1.gamma: 256\n",
      "layers.4.norm1.beta: 256\n",
      "layers.4.norm2.gamma: 256\n",
      "layers.4.norm2.beta: 256\n",
      "layers.5.attention.combine_heads.bias: 256\n",
      "layers.5.ff.net.2.bias: 256\n",
      "layers.5.norm1.gamma: 256\n",
      "layers.5.norm1.beta: 256\n",
      "layers.5.norm2.gamma: 256\n",
      "layers.5.norm2.beta: 256\n",
      "layers.6.attention.combine_heads.bias: 256\n",
      "layers.6.ff.net.2.bias: 256\n",
      "layers.6.norm1.gamma: 256\n",
      "layers.6.norm1.beta: 256\n",
      "layers.6.norm2.gamma: 256\n",
      "layers.6.norm2.beta: 256\n",
      "layers.7.attention.combine_heads.bias: 256\n",
      "layers.7.ff.net.2.bias: 256\n",
      "layers.7.norm1.gamma: 256\n",
      "layers.7.norm1.beta: 256\n",
      "layers.7.norm2.gamma: 256\n",
      "layers.7.norm2.beta: 256\n",
      "layers.8.attention.combine_heads.bias: 256\n",
      "layers.8.ff.net.2.bias: 256\n",
      "layers.8.norm1.gamma: 256\n",
      "layers.8.norm1.beta: 256\n",
      "layers.8.norm2.gamma: 256\n",
      "layers.8.norm2.beta: 256\n",
      "layers.9.attention.combine_heads.bias: 256\n",
      "layers.9.ff.net.2.bias: 256\n",
      "layers.9.norm1.gamma: 256\n",
      "layers.9.norm1.beta: 256\n",
      "layers.9.norm2.gamma: 256\n",
      "layers.9.norm2.beta: 256\n",
      "layers.10.attention.combine_heads.bias: 256\n",
      "layers.10.ff.net.2.bias: 256\n",
      "layers.10.norm1.gamma: 256\n",
      "layers.10.norm1.beta: 256\n",
      "layers.10.norm2.gamma: 256\n",
      "layers.10.norm2.beta: 256\n",
      "layers.11.attention.combine_heads.bias: 256\n",
      "layers.11.ff.net.2.bias: 256\n",
      "layers.11.norm1.gamma: 256\n",
      "layers.11.norm1.beta: 256\n",
      "layers.11.norm2.gamma: 256\n",
      "layers.11.norm2.beta: 256\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "parameter_to_data_ratio = n_params / len(train_data)\n",
    "print(f\"{parameter_to_data_ratio=}\")\n",
    "\n",
    "parameters = []\n",
    "for name, param in model.named_parameters():\n",
    "    parameters.append({\"name\": name, \"params\": param.numel()})\n",
    "\n",
    "# sort parameters by size\n",
    "sorted_parameters = sorted(parameters, key=lambda x: x[\"params\"], reverse=True)\n",
    "for p in sorted_parameters:\n",
    "    print(f\"{p['name']}: {p['params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/60000 [00:01<8:27:25,  1.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 2.429755449295044, 'val': 2.431878089904785, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 302/60000 [00:36<4:44:56,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.8367623686790466, 'val': 0.8470489382743835, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 602/60000 [01:18<4:41:42,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.7151026129722595, 'val': 0.7169919013977051, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 902/60000 [02:03<3:51:03,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.6502602696418762, 'val': 0.6510316133499146, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1202/60000 [02:44<3:03:09,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.6058956980705261, 'val': 0.605420708656311, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1502/60000 [03:26<4:49:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5789751410484314, 'val': 0.5838618278503418, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1803/60000 [04:12<2:46:37,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5555378794670105, 'val': 0.557843804359436, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2103/60000 [04:50<2:54:14,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5497593283653259, 'val': 0.5462676882743835, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2403/60000 [05:28<2:34:13,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5319753289222717, 'val': 0.5288966298103333, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2703/60000 [06:07<3:09:55,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5249441862106323, 'val': 0.5181480050086975, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3002/60000 [06:49<4:36:09,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5122328996658325, 'val': 0.5077174305915833, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3302/60000 [07:31<4:37:28,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5039597153663635, 'val': 0.5057711601257324, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3602/60000 [08:10<3:50:07,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5024383664131165, 'val': 0.49779969453811646, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3902/60000 [08:48<3:13:10,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4950229525566101, 'val': 0.49174731969833374, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4203/60000 [09:26<2:31:15,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.48267045617103577, 'val': 0.4901937246322632, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4502/60000 [10:08<4:50:28,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.486367791891098, 'val': 0.4832426607608795, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4803/60000 [10:40<2:24:08,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.47537341713905334, 'val': 0.4770694077014923, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 5102/60000 [11:20<4:14:24,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.47356557846069336, 'val': 0.4775037467479706, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5402/60000 [12:04<3:14:06,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.46566274762153625, 'val': 0.463899165391922, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 5702/60000 [12:42<4:10:09,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4797123670578003, 'val': 0.46505361795425415, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6002/60000 [13:26<4:10:52,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4616134464740753, 'val': 0.46008652448654175, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6302/60000 [14:10<4:19:04,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4644951820373535, 'val': 0.4598516523838043, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6602/60000 [14:46<3:21:03,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.45522618293762207, 'val': 0.4626397490501404, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6902/60000 [15:22<4:16:21,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.46410071849823, 'val': 0.458983451128006, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7203/60000 [16:05<2:32:46,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.45678386092185974, 'val': 0.45689690113067627, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7502/60000 [16:45<4:19:21,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.44554781913757324, 'val': 0.4515269100666046, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7802/60000 [17:21<4:09:26,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.450727641582489, 'val': 0.45100945234298706, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 8102/60000 [18:00<3:20:54,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4468293786048889, 'val': 0.4453596770763397, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8402/60000 [18:43<4:13:52,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.44623324275016785, 'val': 0.4518088698387146, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 8702/60000 [19:21<2:26:37,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4412625730037689, 'val': 0.45233696699142456, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9003/60000 [19:52<2:19:40,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4462968409061432, 'val': 0.4504774808883667, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 9302/60000 [20:23<2:26:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4341552257537842, 'val': 0.4360671639442444, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 9602/60000 [20:57<4:11:11,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.44603899121284485, 'val': 0.4409247636795044, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 9902/60000 [21:42<4:03:39,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.437929630279541, 'val': 0.4411149024963379, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10202/60000 [22:20<3:18:26,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.43779417872428894, 'val': 0.4368560314178467, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 10502/60000 [23:03<3:30:39,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4347199499607086, 'val': 0.438816100358963, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 10802/60000 [23:47<3:48:52,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4374655783176422, 'val': 0.43755337595939636, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 11102/60000 [24:25<4:02:45,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4372262954711914, 'val': 0.43769925832748413, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 11402/60000 [25:01<2:37:23,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4402065575122833, 'val': 0.4414510130882263, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 11702/60000 [25:38<2:44:17,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.43819543719291687, 'val': 0.4373345375061035, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12002/60000 [26:24<2:27:06,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.42662832140922546, 'val': 0.4348389804363251, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 12302/60000 [26:58<3:25:25,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.42912256717681885, 'val': 0.4344235062599182, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 12603/60000 [27:47<2:25:22,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4255281984806061, 'val': 0.4332534968852997, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 12902/60000 [28:29<2:51:21,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4269726872444153, 'val': 0.4232653081417084, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13202/60000 [29:11<2:56:52,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.42444613575935364, 'val': 0.4320295751094818, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 13502/60000 [29:52<3:15:07,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4284628629684448, 'val': 0.4298068583011627, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 13802/60000 [30:31<3:21:12,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.42587125301361084, 'val': 0.43010571599006653, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 14102/60000 [31:17<2:38:43,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.41953179240226746, 'val': 0.4261939525604248, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 14403/60000 [31:55<2:11:09,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.43378379940986633, 'val': 0.42606818675994873, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 14702/60000 [32:31<2:21:18,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4257335960865021, 'val': 0.43004879355430603, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15002/60000 [33:09<2:46:57,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4245215058326721, 'val': 0.4223972260951996, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 15302/60000 [33:52<2:54:55,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.42161744832992554, 'val': 0.426285058259964, 'kurtosis_avg': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 15496/60000 [34:14<1:38:20,  7.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# l2 regularization\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# l2 = sum(p.pow(2).sum() for p in model.parameters()) / num_params\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m kurtosis_avg \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.4\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m160\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/_tensor.py:513\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/overrides.py:1604\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1604\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "import tqdm\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "for steps in tqdm.tqdm(range(max_iters)):\n",
    "    xb, yb = get_batch('train')\n",
    "    # loss\n",
    "    logits, loss, kurtosis_avg = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # l2 regularization\n",
    "    # l2 = sum(p.pow(2).sum() for p in model.parameters()) / num_params\n",
    "    loss = loss + kurtosis_avg * 0.4/160\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        # wandb.log({\"tIain\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "        print({\"tIain\": losses['train'].item(), \"val\": losses['val'].item(), \"kurtosis_avg\": kurtosis_avg.item()})\n",
    "\n",
    "losses = estimate_loss(is_last=True)\n",
    "# wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.4164, device='cuda:0'),\n",
       " 'val': tensor(0.4204, device='cuda:0')}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model\n",
    "# torch.save(model.state_dict(), 'tiny-stories-model.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load('tiny-stories-model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lilly saw a big red apple. She wanted to eat it, but she didn't know how. She tried to take a bite, but it was too high.\n",
      "\n",
      "Suddenly, Lilly saw a little bird. The bird was stuck in a tree. Lilly knew she had to rescue the bird. She climbed up the tree and carefully climbed up. The bird was safe and Lilly felt happy.\n",
      "\n",
      "Lilly learned that sometimes it's good to ask for help when you need it. She also learned that it's important to always ask for help when you need it. Lilly was proud of herself for being brave and kind to the bird.\"\n",
      "\"Once upon a time, there was a girl named Lily. She loved to play outside in the sun. One day, she went to the park to play. She saw a big tree and wanted to climb it. But she was scared because she was scared.\n",
      "\n",
      "Lily saw a bird and said, \"\"Hi, bird! Can you help me climb the tree?\"\" The bird replied\n"
     ]
    }
   ],
   "source": [
    "print(model.prompt_model(\"Lilly saw a big red apple.\", 200, 0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurtosis debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the highest value in emb1: 186\n",
      "Index of the highest value in emb2: 186\n",
      "Index of the lowest value in emb1: 171\n",
      "Index of the lowest value in emb2: 171\n",
      "emb1 excess kurtosis: 157.1050262451172\n",
      "emb2 excess kurtosis: 156.85986328125\n",
      "Dot product between emb1 and emb2: 0.9625234007835388\n"
     ]
    }
   ],
   "source": [
    "story1='''Once upon a time, in a big forest, there lived a rhinoceros named Roxy. Roxy loved to climb. She climbed trees, rocks, and hills. One day, Roxy found an icy hill. She had never seen anything like it before. It was shiny and cold, and she wanted to climb it.\n",
    "Roxy tried to climb the icy hill, but it was very slippery. She tried again and again, but she kept falling down. Roxy was sad. She wanted to climb the icy hill so much. Then, she saw a little bird named Billy. Billy saw that Roxy was sad and asked, \"Why are you sad, Roxy?\"\n",
    "Roxy told Billy about the icy hill and how she couldn't climb it'''\n",
    "\n",
    "# assume BxTxC\n",
    "def excess_kurtosis(emb):\n",
    "    mean = torch.mean(emb, dim=-1, keepdim=True) # BxTx1\n",
    "    std = torch.std(emb, dim=-1, keepdim=True) # BxTx1\n",
    "\n",
    "    centralized = emb - mean #BxTxC\n",
    "    fourth_moment = torch.mean(centralized**4, dim=-1, keepdim=True) # BxTx1\n",
    "    kurtosis = torch.squeeze(fourth_moment / std**4, dim=-1) # BxT\n",
    "    return kurtosis - 3\n",
    "\n",
    "\n",
    "\n",
    "emb1 = model.get_embedding(\"Tim and Lily saw a big dog\", override_model_embedding_layer=6)\n",
    "emb2 = model.get_embedding(\"Tim and Lily noticed a cat\", override_model_embedding_layer=6)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Plot emb1 and emb2 in the same plot\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(np.square(emb1.cpu().detach().numpy()), label='emb1', color='blue')\n",
    "# plt.plot(np.square(emb2.cpu().detach().numpy()), label='emb2', color='red')\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('emb1 and emb2 Plot')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# get the index of the highest value\n",
    "# Assuming emb1 and emb2 are tensors\n",
    "highest_value_index_emb1 = torch.argmax(emb1).item()\n",
    "highest_value_index_emb2 = torch.argmax(emb2).item()\n",
    "\n",
    "lowest_value_index_emb1 = torch.argmin(emb1).item()\n",
    "lowest_value_index_emb2 = torch.argmin(emb2).item()\n",
    "\n",
    "print(f\"Index of the highest value in emb1: {highest_value_index_emb1}\")\n",
    "print(f\"Index of the highest value in emb2: {highest_value_index_emb2}\")\n",
    "print(f\"Index of the lowest value in emb1: {lowest_value_index_emb1}\")\n",
    "print(f\"Index of the lowest value in emb2: {lowest_value_index_emb2}\")\n",
    "\n",
    "print(f\"emb1 excess kurtosis: {excess_kurtosis(emb1)}\")\n",
    "print(f\"emb2 excess kurtosis: {excess_kurtosis(emb2)}\")\n",
    "\n",
    "# dot product between emb1 and emb2\n",
    "emb1_l2 = F.normalize(emb1, p=2, dim=-1)\n",
    "emb2_l2 = F.normalize(emb2, p=2, dim=-1)\n",
    "print(f\"Dot product between emb1 and emb2: {torch.dot(emb1_l2, emb2_l2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emb1 excess kurtosis: 157.1050262451172\n",
    "emb2 excess kurtosis: 156.85986328125\n",
    "when we load the model trained from this notebook, it has excess kurtosis of 157.1050262451172"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
